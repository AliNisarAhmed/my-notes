# Growth of Functions 

## Big O Notation

- let $f$ and $g$ be functions from $\Z$ or $\reals$ to $\reals$ , we say that $f(x)$ is $O(g(x))$ if $\exist C$ and $\exist k$ such that: 
    
        $|f(x)| ≤ C|g(x)|$

    whenever x > k. 

- Another way of stating the above: 

    **Upper bound** $f(n) = O(g(n))$ means $c . g(n)$ is an upper bound on $f(n)$
        - In words, there exists some constant $c$ such that $f(n)$ is always $<=c.g(n)$, for large enough $n$ > $n_0$, so once $n$ crosses $n_0$, the upper bound function always remains above $f(n)$.

- The constants $C$ and $k$ are called ___Witnesses___

- if $f(x) = O(g(x))$ and $g(x) = O(f(x))$ , we say that the functions $f$ and $g$ are of the **same order**

Examples

- show that $7x^2$ is $O(x^3)$ 
    - note that when $x > 7$ (means $k = 7$) 
        - multiply both sides by $x^2$
        - we get $x^3 > 7x^2$ 
        - Hence $k = 7$ and $C = 1$ are witnesses
    - also note when $x > 1$ 
        - multiple both side by $7x^2$ 
        - we get $7x^3 > 7x^2$ 
        - thus, $k = 1$ and $C = 7$ are also witnesses
    
- show that $x^3$ is not $O(7x^2)$
    - Using proof by contradiction
        - assume $C$ and $k$ are witnesses 
        - then $x^3 < C.7x^2$ for all $x > k$ 
        - simplifying, we get $x < 7C$ 
        - However, no matter what $C$ is, we can always make $x$ greater than $7C$ (because any arbitrarily large values of $x$ can be chosen, as long as it is > k)
        - It follows that no such witnesses can exist.

### Big-O for Polynomials 

![3a6a4d1a90b98c0221672496a6f2f4e7.png](3a6a4d1a90b98c0221672496a6f2f4e7.png)

### Other functions 

- Each function in the list below is smaller than the succeeding function

$1$ , $log n$ , $\sqrt{n}$ , $n$ , $n log n$ , $n^{3/2}$ , $n^2$ , $2^n$ , $n!$  

- $x!$ is $O(x^x)$ 
- $log (x!)$ is $O(n log n)$ 

- $x^c$ is $O(x^d)$ (if $d > c > 1$ ) , but $x^d$ is not $O(n^c)$. 
- $(log_b x)^c$ is $O(x^d)$ , but $x^d$ is not $O((log_b x )^c)$ 
- $x^d$ is $O(b^n)$ , but $b^n$ is not $O(x^d)$ 
- $b^x$ is $O(c^x)$ (when $c > b > 1$), but , $c^x$ is not $O(b^x)$ 
- $c^x$ is $O(n!)$ , but $n!$ is not $O(c^x)$  

### Big-O for addition of two functions 

![09136022caa932409a8891978db46dd1.png](09136022caa932409a8891978db46dd1.png)

from the above, this corollary follow: 

![bd5567e5719e1f9c8cd8d24986782fb5.png](bd5567e5719e1f9c8cd8d24986782fb5.png)

Example: 

![cb9cd574a41bd68a0c2021e7e4d56fcc.png](cb9cd574a41bd68a0c2021e7e4d56fcc.png)

### Big-O for multiplication of two functions 

![be79567fc5ca561ba013a7dc134a99d4.png](be79567fc5ca561ba013a7dc134a99d4.png)
 
 Example
 
 ![b073fa086120b7a022ffc2fbc5d2bc52.png](b073fa086120b7a022ffc2fbc5d2bc52.png)

## Big-Omega Notation

Big-Omega describes the lower bound of functions, thus below, $g$ is lower bound of $f$ 

![9486202bce1048fcb4dc74940c073ebe.png](9486202bce1048fcb4dc74940c073ebe.png)

Another way of saying it

- **Lower bound** $f(n) = \Omega (g(n))$ means $c.g(n)$ is a lower bound on $f(n)$
    - Thus, there exists some constant $c$ such that $f(n)$ is always >= $c.g(n)$ for all $n > n_0$


There is a strong connection between big-O and big-Omega notation. In particular, f(x) is Ω(g(x)) if and only if g(x) is O(f(x)). 

## Big-Theta Notation 

![ed9c6a252639a637ce3399b60940e781.png](ed9c6a252639a637ce3399b60940e781.png)

***

## Worst-case Analysis

By the worst-case performance of an algorithm, we mean the largest number of operations needed to solve the given problem using this algorithm on input of specified size. 

Worst-case analysis tells us how many operations an algorithm requires to guarantee that it will produce a solution.

## Average-case Analysis 

Another important type of complexity analysis, besides worst-case analysis, is called **average-case analysis**. 

The average number of operations used to solve the problem over all possible inputs of a given size is found in this type of analysis. 

Average-case time complexity analysis is usually much more complicated than worst-case analysis

## Complexity of Matrix multiplication

Suppose: 
    - $A = [a_{ij}]$ (order = $m \times k$)
    - $B = [b_{ij}]$ (order = $k \times n$)
    - $C = A \times B = [c_{ij}]$ (order = $m \times n$)

Algorithm for multiplication: 

```js
procedure matrix_multiplication (A, B : matrices) 
    for i := 1 to m 
        for j := 1 to n 
            c[ij] := 0 
            for q := 1 to k // k is number of columns in A and number of rows in B
                c[ij] := c[ij] + a[iq]b[qj]
    return C // C = c[ij] is the product of A and B
```

Q: How many additions and multiplications of integers are used by the above algorithm to multiply two $n \times n$ matrices?

A: 
- There are $n^2$ entries in the product of $A \times B$ (number of rows $n$ times number of columns $n$ of the result)
- To find each entry requires
    - $n$ multiplications
    - $n - 1$ additions (e.g. $1 \times 2 + 3 \times 4 + 5 \times 6$ has $3$ multiplications and $2$ additions)
    - Hence, a total of $n^3$ multiplications and $n^2(n -1)$ additions are used.

Thus the Time complexity of matrix multiplication is $O(n^3)$ (some algorithms allow matrix multiplication in $O(n^{\sqrt{7}}))$ 
